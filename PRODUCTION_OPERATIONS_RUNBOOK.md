# ðŸ“‹ PRODUCTION OPERATIONS RUNBOOK
## Monte Carlo Platform - Enterprise Operations Guide

---

## **ðŸŽ¯ OVERVIEW**

This runbook provides standardized procedures for operating the Monte Carlo Platform in production, including incident response, monitoring, maintenance, and escalation procedures.

**Scope**: Multi-instance production environment with enterprise customers
**Audience**: DevOps, SRE, Support, and Engineering teams
**SLA Targets**: 99.9% uptime, <2s response time, <4h critical issue resolution

---

## **ðŸš¨ INCIDENT RESPONSE PROCEDURES**

### **Incident Classification**

| **Severity** | **Definition** | **Response Time** | **Examples** |
|--------------|----------------|-------------------|--------------|
| **P0 - Critical** | Service completely down | **15 minutes** | API gateway down, database outage |
| **P1 - High** | Major feature impacted | **1 hour** | Simulation engine failure, auth issues |
| **P2 - Medium** | Minor feature impacted | **4 hours** | Slow responses, non-critical features |
| **P3 - Low** | Cosmetic or minor issues | **24 hours** | UI glitches, documentation errors |

### **ðŸ”¥ P0 - CRITICAL INCIDENT RESPONSE**

#### **Immediate Actions (0-15 minutes)**
```bash
# 1. Acknowledge and assess
â–¡ Update status page: https://status.montecarloanalytics.com
â–¡ Create incident channel: #incident-YYYY-MM-DD-HH-MM
â–¡ Page on-call engineer via PagerDuty
â–¡ Begin incident log documentation

# 2. Initial diagnosis
â–¡ Check system health dashboard: http://localhost:3001/dashboard/overview
â–¡ Verify infrastructure status: kubectl get pods --all-namespaces
â–¡ Check external dependencies: Redis, PostgreSQL, monitoring stack
â–¡ Review recent deployments and changes
```

#### **Detailed Response Actions (15-60 minutes)**
```bash
# 3. Technical investigation
â–¡ Analyze logs: ELK stack http://localhost:5601
â–¡ Check metrics: Prometheus http://localhost:9090
â–¡ Review traces: Jaeger http://localhost:16686
â–¡ Validate database connectivity and performance

# 4. Communication
â–¡ Update status page with initial findings
â–¡ Notify customer success team for enterprise customers
â–¡ Brief executive team for extended outages (>30 minutes)
â–¡ Post updates every 15 minutes during active incident
```

#### **Resolution and Recovery (60+ minutes)**
```bash
# 5. Resolution implementation
â–¡ Apply fixes (with change control approval for production)
â–¡ Verify fix in staging environment first (if possible)
â–¡ Monitor system stability post-fix
â–¡ Validate all core functionality restored

# 6. Post-incident activities
â–¡ Update status page with "Monitoring" status
â–¡ Schedule post-mortem within 24 hours
â–¡ Document lessons learned
â–¡ Implement preventive measures
```

### **ðŸš¨ ESCALATION MATRIX**

```bash
# Primary On-Call (24/7)
â”œâ”€â”€ L1 Support Engineer (0-15 min response)
â”œâ”€â”€ L2 Platform Engineer (15-60 min response)
â”œâ”€â”€ L3 Senior Engineer (1-4 hour response)
â””â”€â”€ Engineering Manager (4+ hour or customer escalation)

# Executive Escalation (for major outages)
â”œâ”€â”€ CTO (>1 hour P0 incident)
â”œâ”€â”€ CEO (>4 hour P0 incident or major customer impact)
â””â”€â”€ Legal/PR (security incidents or regulatory issues)

# Customer Communication
â”œâ”€â”€ Customer Success (all P0/P1 incidents affecting enterprise customers)
â”œâ”€â”€ Sales Engineering (prospects during trial periods)
â””â”€â”€ Executive Sponsor (for strategic accounts)
```

---

## **ðŸ“Š MONITORING & ALERTING**

### **Critical System Metrics**

#### **Application Health Monitoring**
```bash
# API Response Time Alerts
Alert: API response time > 2 seconds (95th percentile)
Action: Page on-call engineer
Dashboard: Grafana - API Performance

Alert: API error rate > 1%
Action: Create incident ticket
Dashboard: Grafana - Error Rates

Alert: Simulation queue backlog > 100 jobs
Action: Scale simulation workers
Dashboard: Grafana - Queue Metrics
```

#### **Infrastructure Monitoring**
```bash
# Kubernetes Cluster Health
Alert: Pod restart count > 5 in 1 hour
Action: Investigate and potentially scale
Dashboard: Grafana - Kubernetes Overview

Alert: Memory usage > 80%
Action: Scale horizontally or investigate memory leaks
Dashboard: Grafana - Resource Utilization

Alert: Disk usage > 85%
Action: Immediate storage expansion
Dashboard: Grafana - Storage Metrics
```

#### **Database Monitoring**
```bash
# PostgreSQL Performance
Alert: Database connection pool > 80% utilized
Action: Scale connection pool or investigate long-running queries
Query: SELECT * FROM pg_stat_activity WHERE state = 'active';

Alert: Slow query detected (>5 seconds)
Action: Investigate query performance
Tool: pg_stat_statements analysis

Alert: Database replication lag > 30 seconds
Action: Check replication health immediately
Dashboard: Grafana - Database Replication
```

### **Business Metrics Monitoring**
```bash
# Customer Experience Metrics
Alert: Simulation failure rate > 5%
Action: Investigate Ultra engine and GPU allocation
Dashboard: Grafana - Simulation Success Rate

Alert: User signup conversion < 15%
Action: Notify product team for UX investigation
Dashboard: Grafana - Conversion Metrics

Alert: Enterprise customer billing processing failure
Action: Immediate investigation and customer notification
Dashboard: Stripe Dashboard + Internal Analytics
```

---

## **ðŸ”§ MAINTENANCE PROCEDURES**

### **Weekly Maintenance Tasks**

#### **Security Maintenance**
```bash
# Every Monday 6 AM UTC
â–¡ Review security logs for anomalies
â–¡ Update dependency vulnerability reports
â–¡ Rotate non-critical API keys
â–¡ Review user access permissions

# Security checklist
audit_security_logs.py --last-week
check_dependency_vulnerabilities.sh
rotate_api_keys.py --non-critical
review_user_permissions.py --inactive-users
```

#### **Performance Optimization**
```bash
# Every Wednesday 6 AM UTC
â–¡ Analyze slow query reports
â–¡ Review Redis memory usage and eviction policies
â–¡ Optimize database indexes based on query patterns
â–¡ Clear unnecessary log files and temporary data

# Performance checklist
analyze_slow_queries.py --last-week
optimize_redis_memory.py
update_database_indexes.py --analyze-only
cleanup_logs.py --older-than-30-days
```

### **Monthly Maintenance Tasks**

#### **Disaster Recovery Testing**
```bash
# First Saturday of each month
â–¡ Test database backup restoration
â–¡ Validate cross-region failover procedures
â–¡ Test incident response communication channels
â–¡ Review and update runbook procedures

# DR testing checklist
test_database_restore.py --to-staging
test_regional_failover.py --dry-run
test_incident_channels.py
update_runbook_docs.py
```

#### **Capacity Planning Review**
```bash
# Monthly capacity assessment
â–¡ Analyze growth trends and resource utilization
â–¡ Project capacity needs for next 3 months
â–¡ Review auto-scaling configuration
â–¡ Plan infrastructure scaling activities

# Capacity planning tools
capacity_analysis.py --last-month
generate_growth_projections.py
review_autoscaling_metrics.py
plan_infrastructure_scaling.py
```

---

## **ðŸ’¾ BACKUP & DISASTER RECOVERY**

### **Backup Procedures**

#### **Database Backups**
```bash
# Automated daily backups (2 AM UTC)
pg_dump -h $DB_HOST -U $DB_USER -d monte_carlo_prod > backup_$(date +%Y%m%d).sql
aws s3 cp backup_$(date +%Y%m%d).sql s3://mc-backups/database/

# Backup retention policy
â”œâ”€â”€ Daily backups: Retained for 30 days
â”œâ”€â”€ Weekly backups: Retained for 12 weeks  
â”œâ”€â”€ Monthly backups: Retained for 12 months
â””â”€â”€ Yearly backups: Retained for 7 years

# Backup validation (weekly)
test_restore_backup.py --latest-backup --to-staging
validate_backup_integrity.py --all-recent-backups
```

#### **File Storage Backups**
```bash
# User file backups (continuous replication)
aws s3 sync s3://mc-user-files s3://mc-user-files-backup --delete

# Configuration backups (daily)
kubectl get all --all-namespaces -o yaml > k8s_config_$(date +%Y%m%d).yaml
tar -czf config_backup_$(date +%Y%m%d).tar.gz k8s_config_$(date +%Y%m%d).yaml
```

### **Disaster Recovery Procedures**

#### **RTO/RPO Targets**
- **Recovery Time Objective (RTO)**: 4 hours for complete service restoration
- **Recovery Point Objective (RPO)**: 1 hour maximum data loss
- **Service Degradation Threshold**: 2 hours for partial service restoration

#### **Regional Failover Procedure**
```bash
# 1. Assess regional outage scope
â–¡ Verify primary region unavailability
â–¡ Check secondary region readiness
â–¡ Notify customers of potential service disruption

# 2. Execute failover
â–¡ Update DNS records to point to secondary region
â–¡ Restore latest database backup to secondary region
â–¡ Start application services in secondary region
â–¡ Validate service functionality

# 3. Monitor and communicate
â–¡ Monitor service health in secondary region
â–¡ Update status page with recovery progress
â–¡ Notify customers when service is fully restored
â–¡ Begin planning for primary region recovery
```

---

## **ðŸŽ¯ CUSTOMER SUPPORT PROCEDURES**

### **Support Tier Definitions**

| **Tier** | **Response Time** | **Scope** | **Escalation** |
|----------|------------------|-----------|----------------|
| **T1 - Basic** | 2 hours | Account issues, basic usage | T2 after 4 hours |
| **T2 - Technical** | 1 hour | API issues, simulation problems | T3 after 2 hours |
| **T3 - Expert** | 30 minutes | Complex technical issues | Engineering after 1 hour |

### **Common Issue Resolution**

#### **Authentication Problems**
```bash
# User cannot log in
â–¡ Check user account status: SELECT * FROM users WHERE email = ?
â–¡ Verify OAuth provider connectivity
â–¡ Check for account lockout: SELECT failed_login_attempts FROM user_auth WHERE user_id = ?
â–¡ Reset user password if needed: reset_user_password.py --user-id X

# SSO integration issues
â–¡ Validate SAML/OAuth configuration
â–¡ Check enterprise SSO provider status
â–¡ Verify user exists in enterprise directory
â–¡ Test SSO configuration: test_sso_config.py --org-id X
```

#### **Simulation Performance Issues**
```bash
# Slow simulation execution
â–¡ Check GPU utilization: nvidia-smi
â–¡ Verify simulation queue status: redis-cli LLEN simulation_queue
â–¡ Check for resource constraints: kubectl top pods
â–¡ Review simulation complexity: analyze_simulation.py --sim-id X

# Failed simulations
â–¡ Check simulation logs: kubectl logs simulation-worker-X
â–¡ Verify input file integrity: validate_excel_file.py --file-id X
â–¡ Check Monte Carlo parameters: validate_simulation_config.py --config X
â–¡ Test with simplified parameters: test_simulation.py --basic-config
```

#### **Billing and Subscription Issues**
```bash
# Payment failures
â–¡ Check Stripe payment status: stripe payments list --customer X
â–¡ Verify subscription status: SELECT * FROM subscriptions WHERE user_id = ?
â–¡ Check for expired payment methods
â–¡ Process manual payment if needed: process_manual_payment.py --user-id X

# Usage limit exceeded
â–¡ Check current usage: get_user_usage.py --user-id X --month current
â–¡ Verify subscription tier limits: get_subscription_limits.py --user-id X
â–¡ Offer upgrade or temporary limit increase
â–¡ Process subscription upgrade: upgrade_subscription.py --user-id X --tier pro
```

---

## **ðŸ”„ DEPLOYMENT PROCEDURES**

### **Production Deployment Checklist**

#### **Pre-Deployment Validation**
```bash
# Code quality checks
â–¡ All tests passing: pytest backend/tests/ --cov=80
â–¡ Security scan completed: bandit -r backend/
â–¡ Dependency vulnerability check: safety check
â–¡ Load testing passed: k6 run --vus 1000 --duration 5m load-test.js

# Staging validation
â–¡ Full functionality test in staging
â–¡ Performance benchmarks meet SLA requirements
â–¡ Database migration tested (if applicable)
â–¡ Rollback procedure verified
```

#### **Deployment Execution**
```bash
# Blue-green deployment process
â–¡ Deploy to green environment
â–¡ Run smoke tests on green environment
â–¡ Switch traffic gradually (10%, 50%, 100%)
â–¡ Monitor error rates and performance metrics
â–¡ Keep blue environment ready for immediate rollback

# Database migrations (if required)
â–¡ Create database backup before migration
â–¡ Execute migration in maintenance window
â–¡ Validate data integrity post-migration
â–¡ Update application configuration
```

### **Emergency Rollback Procedure**
```bash
# Immediate rollback triggers
- Error rate > 5% for more than 5 minutes
- Response time > 5 seconds for more than 2 minutes  
- Critical feature completely non-functional
- Security vulnerability discovered in new release

# Rollback execution
â–¡ Switch traffic back to previous version
â–¡ Rollback database changes (if safe)
â–¡ Notify engineering team of rollback
â–¡ Schedule immediate post-mortem
â–¡ Communicate with affected customers
```

---

## **ðŸ“ž CONTACT INFORMATION**

### **Emergency Contacts**
```bash
# Primary On-Call (24/7)
â”œâ”€â”€ Phone: +1-XXX-XXX-XXXX
â”œâ”€â”€ Email: oncall@montecarloanalytics.com
â”œâ”€â”€ Slack: #on-call-primary
â””â”€â”€ PagerDuty: https://montecarloanalytics.pagerduty.com

# Escalation Contacts
â”œâ”€â”€ Engineering Manager: +1-XXX-XXX-XXXX
â”œâ”€â”€ CTO: +1-XXX-XXX-XXXX
â”œâ”€â”€ CEO: +1-XXX-XXX-XXXX
â””â”€â”€ Customer Success: +1-XXX-XXX-XXXX

# External Vendors
â”œâ”€â”€ Cloud Provider Support: [Cloud provider emergency number]
â”œâ”€â”€ Security Vendor: [Security vendor 24/7 number]
â”œâ”€â”€ Database Vendor: [Database vendor support]
â””â”€â”€ Monitoring Vendor: [Monitoring vendor support]
```

### **Internal Communication Channels**
```bash
# Slack Channels
â”œâ”€â”€ #incidents - Active incident coordination
â”œâ”€â”€ #alerts - Automated monitoring alerts
â”œâ”€â”€ #deployments - Deployment notifications
â”œâ”€â”€ #customer-escalations - Customer issue escalations
â””â”€â”€ #executive-notifications - Executive updates

# Documentation Links
â”œâ”€â”€ System Architecture: http://docs.internal/architecture
â”œâ”€â”€ API Documentation: http://localhost:8080/docs
â”œâ”€â”€ Monitoring Dashboards: http://localhost:3001
â””â”€â”€ Status Page: https://status.montecarloanalytics.com
```

---

## **ðŸ“š APPENDIX: USEFUL COMMANDS**

### **System Health Checks**
```bash
# Quick system overview
kubectl get pods --all-namespaces | grep -v Running
docker ps --filter "status=exited"
curl -s http://localhost:8000/health | jq .

# Database health
psql -h $DB_HOST -U $DB_USER -c "SELECT version();"
redis-cli ping
curl -s http://localhost:9200/_cluster/health | jq .

# Performance metrics
kubectl top nodes
kubectl top pods --all-namespaces
iostat -x 1 3
```

### **Log Analysis**
```bash
# Application logs
kubectl logs -f deployment/monte-carlo-backend --tail=100
journalctl -u docker -f
tail -f /var/log/nginx/access.log

# Error investigation
grep -i error /var/log/monte-carlo/*.log | tail -20
elasticsearch_query.py --query "level:ERROR AND timestamp:>now-1h"
kibana_dashboard.py --dashboard "Error Analysis"
```

---

**ðŸŽ¯ REMEMBER**: This runbook is a living document. Update procedures based on lessons learned from incidents and operational experience. Review and update monthly during team meetings.**

---

*Last Updated: September 18, 2025*
*Next Review: October 18, 2025*
*Owner: DevOps Team*
